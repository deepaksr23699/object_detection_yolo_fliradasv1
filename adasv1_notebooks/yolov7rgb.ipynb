{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -name \"*.cache\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data = {\n",
    "    'train': '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_3/train/rgbwithlabels/images',\n",
    "    'val': '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_3/val/rgbwithlabels/images',\n",
    "    'test': '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_3/video/rgbwithlabels/images',\n",
    "    'nc': 4,\n",
    "    'names': ['person', 'bicycle', 'car', 'dog']\n",
    "}\n",
    "\n",
    "with open('/home/deepaksr/project/project_assignment2/adasv1_notebooks/v7rgb.yaml', 'w') as file:\n",
    "    documents = yaml.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov7'...\n",
      "remote: Enumerating objects: 1197, done.\u001b[K\n",
      "remote: Total 1197 (delta 0), reused 0 (delta 0), pack-reused 1197 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1197/1197), 74.23 MiB | 13.15 MiB/s, done.\n",
      "Resolving deltas: 100% (520/520), done.\n",
      "/home/deepaksr/project/project_assignment2/adasv1_notebooks/yolov7\n",
      "cfg\tdetect.py  hubconf.py  models\t  requirements.txt  tools\t  utils\n",
      "data\texport.py  inference   paper\t  scripts\t    train_aux.py\n",
      "deploy\tfigure\t   LICENSE.md  README.md  test.py\t    train.py\n"
     ]
    }
   ],
   "source": [
    "!# Download YOLOv7 code\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-11 17:07:32--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241011%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241011T113723Z&X-Amz-Expires=300&X-Amz-Signature=f97f2c785e04924979ee8d5dd438157a974c51d240036b33f2b99203987e0015&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-10-11 17:07:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241011%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241011T113723Z&X-Amz-Expires=300&X-Amz-Signature=f97f2c785e04924979ee8d5dd438157a974c51d240036b33f2b99203987e0015&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75587165 (72M) [application/octet-stream]\n",
      "Saving to: â€˜yolov7.ptâ€™\n",
      "\n",
      "yolov7.pt           100%[===================>]  72.08M  33.3MB/s    in 2.2s    \n",
      "\n",
      "2024-10-11 17:07:36 (33.3 MB/s) - â€˜yolov7.ptâ€™ saved [75587165/75587165]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!# Download trained weights\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepaksr/project/project_assignment2/adas_v1/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd /home/deepaksr/project/project_assignment2/adas_v1/yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ v0.1-128-ga207844 torch 2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11011.5MB)\n",
      "                                            CUDA:1 (NVIDIA GeForce RTX 2080 Ti, 11011.5MB)\n",
      "                                            CUDA:2 (NVIDIA GeForce RTX 2080 Ti, 11011.5MB)\n",
      "                                            CUDA:3 (NVIDIA GeForce RTX 2080 Ti, 11011.5MB)\n",
      "\n",
      "Namespace(adam=False, artifact_alias='latest', batch_size=40, bbox_interval=-1, bucket='', cache_images=False, cfg='', data='/home/deepaksr/project/project_assignment2/adasv1_notebooks/v7rgb.yaml', device='0,1,2,3', entity=None, epochs=11, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp31', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=40, upload_dataset=False, v5_metric=False, weights='yolov7.pt', workers=8, world_size=1)\n",
      "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
      "train.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  run_id = torch.load(weights, map_location=device).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None\n",
      "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
      "train.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 12                -1  1         0  models.common.MP                        []                            \n",
      " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
      " 25                -1  1         0  models.common.MP                        []                            \n",
      " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
      " 38                -1  1         0  models.common.MP                        []                            \n",
      " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
      " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
      " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
      " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 76                -1  1         0  models.common.MP                        []                            \n",
      " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
      " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 89                -1  1         0  models.common.MP                        []                            \n",
      " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
      " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
      " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
      "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
      "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
      "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
      "105   [102, 103, 104]  1     48465  models.yolo.Detect                      [4, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
      "/home/deepaksr/miniconda3/envs/yolov5/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 407 layers, 37210865 parameters, 37210865 gradients, 105.2 GFLOPS\n",
      "\n",
      "Transferred 554/560 items from yolov7.pt\n",
      "Scaled weight_decay = 0.000625\n",
      "Optimizer groups: 95 .bias, 95 conv.weight, 92 other\n",
      "/home/deepaksr/project/project_assignment2/adas_v1/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cache, exists = torch.load(cache_path), True  # load\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_3/\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.66, Best Possible Recall (BPR) = 1.0000\n",
      "train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=cuda)\n",
      "Image sizes 640 train, 640 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/exp31\n",
      "Starting training for 11 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "  0%|                                                   | 0/209 [00:00<?, ?it/s]train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=cuda):\n",
      "      0/10     1.61G    0.0769    0.0151    0.0117    0.1037       571       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.331       0.213      0.0436     0.00818\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      1/10     6.88G   0.06674   0.01444  0.006011   0.08719       538       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.347       0.222       0.079      0.0191\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      2/10     7.31G   0.06349   0.01466  0.004762   0.08291       675       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.369      0.0961      0.0543      0.0161\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      3/10     7.22G    0.0605   0.01538  0.004477   0.08036       590       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.412       0.183       0.111      0.0343\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      4/10     8.97G   0.06047   0.01528  0.004625   0.08037       624       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819        0.42       0.132      0.0806      0.0239\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      5/10     7.29G   0.05848   0.01561  0.004303   0.07839       595       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.469       0.208       0.153      0.0518\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      6/10      7.5G   0.05594   0.01544  0.003801   0.07518       499       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.458       0.221       0.152      0.0448\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      7/10     8.71G   0.05334   0.01531  0.003206   0.07186       700       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.469       0.249       0.178      0.0596\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      8/10        9G   0.05112   0.01499  0.002813   0.06893       669       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.227       0.226       0.194      0.0658\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      9/10     8.69G   0.04955   0.01502  0.002516   0.06709       574       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.348       0.323       0.275      0.0943\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     10/10     7.51G   0.04817    0.0147   0.00235   0.06522       503       640\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1254       10819       0.456       0.422       0.312       0.116\n",
      "              person        1254        5397       0.252       0.511       0.305      0.0909\n",
      "             bicycle        1254         417       0.219       0.434       0.255       0.069\n",
      "                 car        1254        4992       0.353       0.588       0.532       0.258\n",
      "                 dog        1254          13           1       0.153       0.157      0.0464\n",
      "11 epochs completed in 2.784 hours.\n",
      "\n",
      "/home/deepaksr/project/project_assignment2/adas_v1/yolov7/utils/general.py:802: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs/train/exp31/weights/last.pt, 74.8MB\n",
      "Optimizer stripped from runs/train/exp31/weights/best.pt, 74.8MB\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_size = 640  # Image size\n",
    "batch_size = 40  # Batch size\n",
    "epochs = 11  # Number of epochs\n",
    "data_yaml_path = '/home/deepaksr/project/project_assignment2/adasv1_notebooks/v7rgb.yaml'  # Path to your dataset YAML file\n",
    "weights = 'yolov7.pt'  # Pre-trained weights\n",
    "device = '0,1,2,3'  # Use '0' for first GPU\n",
    "\n",
    "# Run the training script\n",
    "!python train.py --img {img_size} --batch {batch_size} --epochs {epochs} --data {data_yaml_path}  --weights {weights} --device {device}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment=False, batch_size=32, conf_thres=0.25, data='/home/deepaksr/project/project_assignment2/adasv1_notebooks/v7rgb.yaml', device='0', exist_ok=False, img_size=640, iou_thres=0.5, name='exp', no_trace=False, project='runs/test', save_conf=False, save_hybrid=False, save_json=False, save_txt=False, single_cls=False, task='test', v5_metric=False, verbose=False, weights=['/home/deepaksr/project/project_assignment2/adas_v1/yolov7/runs/train/exp31/weights/best.pt'])\n",
      "YOLOR ðŸš€ v0.1-128-ga207844 torch 2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11011.5MB)\n",
      "\n",
      "/home/deepaksr/project/project_assignment2/adas_v1/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "/home/deepaksr/miniconda3/envs/yolov5/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 306 layers, 36496081 parameters, 6194944 gradients, 103.2 GFLOPS\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/home/deepaksr/project/project_assignment2/adas_v1/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cache, exists = torch.load(cache_path), True  # load\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/deepaksr/project/project_assignment2/adas_v1/FLIR_ADAS_1_3\u001b[0m\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        4193       36920       0.388      0.0893      0.0478      0.0178\n",
      "              person        4193       21778       0.288      0.0526      0.0187     0.00407\n",
      "             bicycle        4193        1176       0.472      0.0213      0.0138     0.00259\n",
      "                 car        4193       13966       0.406       0.194       0.111      0.0468\n",
      "Speed: 5.1/1.0/6.1 ms inference/NMS/total per 640x640 image at batch-size 32\n",
      "Results saved to runs/test/exp5\n"
     ]
    }
   ],
   "source": [
    "!python test.py --weights /home/deepaksr/project/project_assignment2/adas_v1/yolov7/runs/train/exp31/weights/best.pt --data /home/deepaksr/project/project_assignment2/adasv1_notebooks/v7rgb.yaml --img 640 --batch 32 --conf 0.25 --task test --device 0 --iou 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
